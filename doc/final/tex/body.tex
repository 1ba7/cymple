\section{Abstract}
Cymple, a recursive acronym which means `Cymple: Your Musical Past Lovingly Envisioned', is a data visualisation application created using Processing, as outlined by the project specification.\cite{specification} The dataset it visualises consists of entries linking a user, a track and a time to mean `user A listened to track B at time C'. It allows the user to interactively select arbitrary subsets of the given dataset and visualises them appropriately. This report outlines the steps that we took to get from the project specification to the final application.

\section{Initial Organisation}
\subsection{Communication}
One of the first things that we did as a team was work out how we were going to communicate with each other. We chose to communicate using email, instant messaging and mobile phones, which all have different merits. Email was convenient for simultaneously informing the group of important information such meeting cancellations or major changes to the codebase. Instant messaging allowed specific group members to discuss ideas more thoroughly than possible in an email. Mobile phones allowed us to organise ad-hoc meetings inside college or find out where somebody was. We figured we would be most effective as a group if we were able to use all of these media.

For email, we first tried setting up a Google Group, but we were unable to do this using our college email accounts. Instead, we set up an email address which forwards all incoming mail to all of our email addresses --- if we wanted to send an email to the group, we would send it to that address. For instant messaging, we simply exchanged our college email addresses and communicated using Google Talk. Some of the group set up their college Google Talk account with desktop-based instant messaging applications such as Pidgin and Trillian. For mobile phones, it was a simple case of exchanging phone numbers.

Although we would see each other during the week anyway, we needed to organise fixed meeting times at which we could discuss ideas in depth. After a discussion while consulting our revised timetable for this term, we chose to meet have meetings at 11:00 -- 13:00 on Mondays and 12:00 -- 13:00 on Thursdays. We wanted to have the meetings somewhere quieter than LG12, so Stiabh suggested the Hamilton library's study room. We were able to book these for the Monday, but not the Thursday. On Thursday, we used ICT 2 which happened to be free at that time.

\subsection{Revision control system}
The next thing that we did was work out what revision control system we were going to use. While there are many reasons for choosing Git\cite{better}, there were two things in particular that convinced us to. Shane's prior experience with Git meant that Git had a smoother learning curve than the alternatives (e.g., SVN). This was because Shane was able to quickly bring the rest of the group up to speed with Git. The other big advantage of Git was GitHub, a web application and service built on top of Git. It gave us a central repository for our codebase and a convenient web interface for nagivating the code and viewing changes. GitHub makes it very easy to see who created a file, who changed it at what time --- in other words, it allowed easily us to see who was working on what at any time.

We set up a GitHub account for each person in the group, as well as a `central' account for the group which would store the main repository.\cite{github} We arranged to have a meeting in M20 at 15:00 on a Wednesday, at which Shane would run a tutorial on Git. The tutorial would include getting Git set up on everybody's laptop as well as demonstrating how to use it. Everybody was given a copy of \mono{msysgit}, \mono{putty} and a guide with screenshots of how to use Git on Windows\cite{guide}. The tutorial involved running through this guide and making sure everybody was comfortable with the concepts behind Git. Once we everyone was set up with Git and we had a communications mechanism in place, it was time to focus on implementing the project specification.\cite{specification}

\section{Design Process}
One of the first things that was discussed was how we were going to structure the project. The structure we chose differs slightly (semantically at least) from the one suggested in the project specification. The structure of our project is based on the Model-View-Controller\cite{mvc} pattern, where the model (the internal representation of the application's state) and the view (the user interface) are isolated components, and the controller responds to events (usually originating in the view) and updates the model appropriately.

The model in our case is what we refer to as the `data layer'. The data layer provides an API to the file which stores the entries and returns meaningful representations of the data. For maximum flexibility, we wanted to write a widget toolkit in Processing that automatically handles the nasty stuff like positions, sizes and event handling. This widget toolkit was our view. Part of this toolkit was an event framework --- we wanted individual widgets to be able to respond to mouse events with Javascript-style \mono{onClick} and \mono{onMouseOver} methods. This event framework was our controller, so for us the view and the controller would be part of the same codebase.

What became clear quite early on in the design process that the most important thing to do was to finalise the visual design of the application itself. What sort of representations of the data the data layer should return (Charts? Graphs?), what widgets should the widget toolkit contain (Scrollbars? Buttons?) and what events should be supported by the event framework (Scrolling? Dragging?) depend totally on the design of the application. The application's visual design was therefore what focused on at our next meeting.

\section{Initial Interface Design}
At the meeting where we discussed the application's visual design, the discussion was based on a paper sketch done by Cris. This sketch was somewhat influenced by Niall O'Hara's application in the sense one of its goals was to support the selection of more subsets than were supported by Niall's application. Most of the refinements which we made to Cris' sketch were aimed at reducing clutter and ambiguity, as well as keeping ease of implementation in mind. We also decided against an interface for selecting time that would require the user to enter the day, month and year separately as we felt this would be cumbersome to use.

\image{stiabh.png}{width=10cm}{The design we came up with, digitised by Stiabh.}

This interface has two panes --- the left one allows the user to select subsets of the data and the right one visualises the selected subset. The user is able to select what users, artists, albums and tracks they want to be in the subset of the data being visualised. The design supports visualising the data with charts and graphs. There is no tool in the left pane for selecting subsets of the data based on time because at the time we thought that we should have a separate interface for time-based selection for charts and graphs, so it would go in the right pane. This is something we changed our mind about later on in the final application, there is what we call a `seeker' at the bottom of the left-pane for time-based selection. What isn't shown in Figure \ref{stiabh.png} is a drop-down menu --- in this design, a drop-down menu would be shown when the user clicked `Chart' which would allow the user to select charts by either User, Artist, Album or Track. We refer to these options as `ChartKeys'.

\subsection{Implications for the code}
The interface which we came up with allows users to select subsets of the data which can contain any subset of users, artists, albums and tracks. This is significant because it is harder to implement a data layer which supports queries with an arbitrary set of users than it is to implement a data layer that can support all users or just one user. The cardinality of the set of all possible subsets of users alone (assuming 65 users) is 3.8\e{19} --- in other words, it is impractical to just pre-calculate the results of all possible queries, they must be dynamically generated. As we wanted to beat Niall's application's 2,000,000 figure, we needed this dynamic generation to be quick, so this interface's design meant that efficiency was a big concern in the data layer.

What this interface meant for the widget toolkit was that we would need some sort of scrollbar and some sort of support for tabs. An interesting problem was getting the semantics of the widgets correct --- was the scrollbar a separate widget or was it all just part of some sort of \mono{ScrollableContainer}? Another thing which this interface necessitated was a statusbar at the bottom, which needed to be updated by the data layer. What we didn't realise at the time was that this would require our application to be threaded --- the data layer must be calculating queries (and thus updating the statusbar's internal state) in a separate thread to the thread that draws the application. This report discusses this in more detail later on.

In terms of the event framework, this interface requires that we have some sort of scrollwheel movement. This is significant because Processing has no support for this out-of-the-box. The scrollbar also requires support for dragging --- the bar itself should be able to be dragged up and down. Our application also used `standard' events like clicking, mouse movement, etc. While Processing does support these, to be able to use them the way we wanted to (i.e., per-widget event handling), it was required to build an abstraction layer on top of this. Another issue was that when the user selects or unselects an artist in the menu in the left pane, that should update the albums which are selectable in the albums tab. This required some sort of event for the selection and unselection of a menu item (which is semantically different from the \mono{onClick} event of a menu item).

\section{Class Hierarchy and Directory Structure}
Now that we had some idea of the requirements of the data layer, the widget toolkit and the event framework, we started drawing up a class hierarchy. This was not done in UML as one might expect, but in our own YAML-based microformat\cite{yaml}. This format could describe classes, interfaces and enums. They were described in terms of their attributes, methods, classes/interfaces which they inherited from and interfaces which they implemented. Each class, interface, attribute and method had an associated bit of text which describes its purpose. Shane then wrote a script in Ruby called \mono{gen-stubs}\cite{stubs}, which generated stub \mono{.java} files, complete with comments, from the YAML descriptions. This can be thought of as a sort of reverse Javadoc process.

We ended up with three YAML files: \mono{data.yml}, \mono{common.yml} and \mono{toolkit.yml}. \mono{data.yml} contained all the classes which were exclusive to the data layer (\mono{Artist}, \mono{User}, etc.) and \mono{toolkit.yml} contained all the classes which were exclusive to the widget toolkit (\mono{Widget}, \mono{Container}, etc.), as well as the event framework (\mono{Mouse}, \mono{Event}). \mono{common.yml} contained abstractions of things which were common to both the data layer and the toolkit. For example, it defined \mono{GraphData} which is a data structure that represents the points on a graph --- a \mono{GraphData} is returned from that data layer and passed to a \mono{Graph} widget, which visualises this data.

\subsection{Processing woes}
However, we soon ran into a problem. We had three YAML files, but Processing only allows a flat directory structure for a `sketch'. What we had was not a sketch though, it was a project. Ideally we would have liked to keep common, data and toolkit classes in separate directories to reinforce the isolation of the model and the view. This was only one of the problems that we were having with using the Processing IDE --- Processing's auto-indentation isn't the best, so it made it hard to write readable, consistently-indented code. Despite supporting them internally, Processing's compiler has no support for Java 5.0 syntax --- enumerated types, generics and foreach loops cannot be used with Processing, features which would be useful to us. And these are problems that exist \emph{after} you get your \mono{.pde} file compiled --- Processing's cryptic error messages make it difficult to even get to that stage.

The solution we came up with was to not use the Processing IDE at all. Instead, we would write the project in Java, using Processing's \mono{core.jar}. \mono{.pde} files ultimately compile down to \mono{.java} files --- the Java we wrote was simply the Java that Processing itself would generate. The main differences were that we had to put a \mono{main} method in a class that extends \mono{PApplet}, and we had to use the \mono{0xhexadecimal} int notation for color literals. The latter turned out to be an advantage as it made it easier to give colors an alpha value than in `pure' Processing.

\section{Data Layer}
When our focused turned to implementing the data layer, it seemed logical that there should be some separation of the code which read and stored the data efficiently in memory and the API which provided an interface to this data for the rest of the application. Much of the auxillary data structures which would be used to initially sort the data would no longer be of use once the data was sorted. Many of these data structures would be very heavy on memory requirements and as we wanted to beat Niall's application's total, we were keeping memory usage in mind. Although these data structures would be garbage collected by the JVM, it still felt somehow dirty that the code for sorting and storing the data wasn't completely separate to the code for reading the data.

What we eventually decided to do about this was to completely separate these two steps, so that the data would already be sorted when the application was run. What this necessitated was that we create our own file format which would store the sorted data in a way that could be easily read by the application, i.e., in a binary file format. We ended up doing this twice --- i.e., our initial file format had some limitations and we replaced it with a better one. Both of these formats, and the reasons why a switch was necessary, are detailed below. We call this file format \mono{cymple.bin}.

\subsection{Initial format}
The initial file format consisted of three parts: the header, the listen indices and the listen blocks. The specifications of these parts are outlined in Tables \ref{header}, \ref{listenindices} and \ref{listenblocks} respectively. The header contained information about the users, artists, albums and tracks in the dataset and the timerange over which the data spanned. What the header didn't contain was any information linking users, tracks and times like how CSV file does -- this is the information that was stored in the rest of the file. The listen blocks store sorted arrays of times (represented as shorts). The listen indices point to the start of the listen block for a given user, user-artist tuple, user-album tuple or user-track tuple. Again, Shane wrote a Ruby script called \mono{gen-bin} to convert from the CSV file into the binary file format\cite{bin}. The script is not very efficient, but it was quick to write and we needed a file in \mono{cymple.bin} format before we could progress any further. The format explained in more detail in the specifications found in the appendix (on page \pageref{appendix}).

As specified by the YAML specification, the data layer needs to be able to return a \mono{ChartData} and a \mono{GraphData} based on an arbitrary set of users and tracks given. It turned out to be impractical (with this file format) to support track-level subsets --- instead the data layer was given an arbitrary set of users and \emph{albums}. When the data layer was given a set of users and albums, it read the listen blocks for each of these users and albums in from the file and sorted them into a huge in-memory array called \mono{listens}\footnote{As it was an array of shorts, the maximum number of listens this data layer could support was 200,000,000 --- the 512MB memory limit is exceeded soon after that.}. What was required for a \mono{GraphData} was a series of points describing the graph. By performing a binary search on \mono{listens} for two given times and subtracting the results, what you are left with is the number of listens between those times. We call this `sampling' and it's how this data layer generated the points (or `samples') for the graph. What was required for a \mono{ChartData} was a list of \mono{String}s (names), a list of \mono{double}s (relative widths of bars) and a list of \mono{int}s (absolute number of listens). The procedure for generating this was complicated, but involved more binary searches.\cite{olddata}

As the application started to come together, we started realising the limitations of this file format. The totals generated with binary searches weren't always accurate and this was embarrassingly obvious on small subsets of the data. As the size of the dataset went into the millions, there would be a huge amount of duplicate values in the \mono{listens} array --- by design, of course, but the idea of having an array whose \emph{keys} are times and values are the number of listens `at' that time seemed more appealing than ever. The biggest problem with this format though came up when we tried to implement the statusbar --- it wasn't designed with concurrency in mind at all. None of the data structures were thread safe and it wasn't clear what which should go in which thread. Because of these limitations, we decided to redo the data layer, more or less from scratch.

\subsection{Revised format}
The new data layer was designed with concurrency in mind (it had its own thread) and was based on the concept of ListenVectors.\cite{newdata} A ListenVector, as described in Table \ref{listenvector}, is a data structure which wraps an array of 1024 8-byte integers (i.e., \mono{long}s). While previously we normalised time to a value between -32768 and 32767, we now normalise time to a value between 0 and 1024. So \mono{listenVector[n]} is the number of listens \emph{before} time n, where n is a value between 0 and 1024. While we lose a lot of accuracy (over a 5-year period, 1 unit of time is roughly 2 days), the total number of listens that our data layer can support is now the largest number that can be stored in a \mono{long} --- in Java's case, 9\e{18}.

Ruby is no longer used to generate \mono{cymple.bin} --- the algorithm for doing so is now much simpler, so it didn't take too long to implement it in Java. The process for converting a CSV file into a \mono{cymple.bin} file has changed somewhat though. There are now two steps --- generating the header (the header format in the revised file format is still the same as in the initial one), which is done by \mono{GenerateHeader.java}\cite{header} and generating the body, which is done by \mono{GenerateBody.java}\cite{body}. The reason for this is that to generate a body from a CSV file with 9\e{18} listens from it would be impossible --- a \mono{dataset.csv} with 200,000,000 entries is 15GB in size. So while \mono{GenerateBody.java} \emph{can} generate the body from a CSV file, it also supports generating a random body without a CSV file, which can generate much larger data sets.












